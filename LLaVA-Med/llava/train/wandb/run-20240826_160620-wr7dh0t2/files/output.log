  0%|                                                                                | 0/28816 [00:00<?, ?it/s]/home/lby/anaconda3/envs/llava-med/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/lby/anaconda3/envs/llava-med/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/lby/anaconda3/envs/llava-med/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
{'loss': 2.7547, 'learning_rate': 2.3121387283236997e-07, 'epoch': 0.0}
  0%|                                                                     | 1/28816 [00:07<56:57:56,  7.12s/it]

  0%|                                                                     | 2/28816 [00:11<44:10:01,  5.52s/it]

  0%|                                                                     | 3/28816 [00:14<33:59:20,  4.25s/it]
[2024-08-26 16:06:42,387] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  0%|                                                                     | 4/28816 [00:17<29:59:06,  3.75s/it]


  0%|                                                                     | 6/28816 [00:22<25:56:22,  3.24s/it]
{'loss': 2.4037, 'learning_rate': 1.3872832369942197e-06, 'epoch': 0.0}

  0%|                                                                     | 7/28816 [00:25<24:35:49,  3.07s/it]

  0%|                                                                     | 8/28816 [00:28<23:34:10,  2.95s/it]

  0%|                                                                     | 9/28816 [00:30<22:38:08,  2.83s/it]

  0%|                                                                    | 10/28816 [00:33<22:31:00,  2.81s/it]

  0%|                                                                    | 11/28816 [00:36<22:37:24,  2.83s/it]

  0%|                                                                    | 12/28816 [00:39<22:28:46,  2.81s/it]

  0%|                                                                    | 13/28816 [00:42<22:37:05,  2.83s/it]

  0%|                                                                    | 14/28816 [00:45<23:07:22,  2.89s/it]

  0%|                                                                    | 15/28816 [00:48<23:15:37,  2.91s/it]

  0%|                                                                    | 16/28816 [00:50<22:37:41,  2.83s/it]

  0%|                                                                    | 17/28816 [00:53<22:30:51,  2.81s/it]

  0%|                                                                    | 18/28816 [00:56<22:08:48,  2.77s/it]

  0%|                                                                    | 19/28816 [00:59<22:33:12,  2.82s/it]
[2024-08-26 16:07:27,524] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  0%|                                                                    | 21/28816 [01:04<22:40:36,  2.84s/it]
{'loss': 2.1722, 'learning_rate': 4.8554913294797685e-06, 'epoch': 0.0}


  0%|                                                                    | 23/28816 [01:11<23:39:13,  2.96s/it]
{'loss': 1.9162, 'learning_rate': 5.317919075144509e-06, 'epoch': 0.0}

  0%|                                                                    | 24/28816 [01:13<23:23:50,  2.93s/it]

  0%|                                                                    | 25/28816 [01:16<23:29:27,  2.94s/it]

  0%|                                                                    | 26/28816 [01:19<22:59:17,  2.87s/it]


  0%|                                                                    | 28/28816 [01:24<22:21:29,  2.80s/it]
{'loss': 2.0402, 'learning_rate': 6.4739884393063585e-06, 'epoch': 0.0}
[2024-08-26 16:07:53,654] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  0%|                                                                    | 29/28816 [01:28<24:06:39,  3.02s/it]

  0%|                                                                    | 30/28816 [01:31<23:51:43,  2.98s/it]


  0%|                                                                    | 32/28816 [01:36<22:57:26,  2.87s/it]
{'loss': 1.7648, 'learning_rate': 7.398843930635839e-06, 'epoch': 0.0}


  0%|                                                                    | 34/28816 [01:42<23:30:04,  2.94s/it]
{'loss': 2.1644, 'learning_rate': 7.861271676300578e-06, 'epoch': 0.0}

  0%|                                                                    | 35/28816 [01:45<23:05:08,  2.89s/it]

  0%|                                                                    | 36/28816 [01:48<22:17:49,  2.79s/it]

  0%|                                                                    | 37/28816 [01:50<22:10:46,  2.77s/it]

  0%|                                                                    | 38/28816 [01:53<22:31:32,  2.82s/it]

  0%|                                                                    | 39/28816 [01:56<21:42:16,  2.72s/it]

  0%|                                                                    | 40/28816 [01:59<22:14:10,  2.78s/it]

  0%|                                                                    | 41/28816 [02:01<22:05:05,  2.76s/it]

  0%|                                                                    | 42/28816 [02:04<22:07:18,  2.77s/it]

  0%|                                                                    | 43/28816 [02:07<22:39:45,  2.84s/it]

  0%|                                                                    | 44/28816 [02:10<22:39:10,  2.83s/it]

  0%|                                                                    | 45/28816 [02:13<22:26:23,  2.81s/it]

  0%|                                                                    | 46/28816 [02:16<22:42:54,  2.84s/it]

  0%|                                                                    | 47/28816 [02:19<22:50:15,  2.86s/it]

  0%|                                                                    | 48/28816 [02:22<23:04:32,  2.89s/it]

  0%|                                                                    | 49/28816 [02:25<23:21:24,  2.92s/it]

  0%|                                                                    | 50/28816 [02:27<22:44:02,  2.85s/it]

  0%|                                                                    | 51/28816 [02:30<22:00:59,  2.76s/it]

  0%|                                                                    | 52/28816 [02:33<22:20:46,  2.80s/it]

  0%|▏                                                                   | 53/28816 [02:35<22:11:48,  2.78s/it]

  0%|▏                                                                   | 54/28816 [02:38<21:42:42,  2.72s/it]

  0%|▏                                                                   | 55/28816 [02:41<21:20:57,  2.67s/it]

  0%|▏                                                                   | 56/28816 [02:43<21:41:12,  2.71s/it]

  0%|▏                                                                   | 57/28816 [02:46<21:06:18,  2.64s/it]

  0%|▏                                                                   | 58/28816 [02:48<20:50:04,  2.61s/it]

  0%|▏                                                                   | 59/28816 [02:51<21:05:22,  2.64s/it]

  0%|▏                                                                   | 60/28816 [02:54<21:25:56,  2.68s/it]

  0%|▏                                                                   | 61/28816 [02:57<21:30:08,  2.69s/it]

  0%|▏                                                                   | 62/28816 [02:59<21:12:03,  2.65s/it]

  0%|▏                                                                   | 63/28816 [03:02<21:30:20,  2.69s/it]

  0%|▏                                                                   | 64/28816 [03:05<22:45:34,  2.85s/it]

  0%|▏                                                                   | 65/28816 [03:08<22:33:01,  2.82s/it]

  0%|▏                                                                   | 66/28816 [03:11<23:02:56,  2.89s/it]

  0%|▏                                                                   | 67/28816 [03:14<22:18:51,  2.79s/it]

  0%|▏                                                                   | 68/28816 [03:16<21:47:45,  2.73s/it]

  0%|▏                                                                   | 69/28816 [03:19<23:00:06,  2.88s/it]

  0%|▏                                                                   | 70/28816 [03:22<22:47:36,  2.85s/it]

  0%|▏                                                                   | 71/28816 [03:25<22:15:44,  2.79s/it]

  0%|▏                                                                   | 72/28816 [03:28<22:29:53,  2.82s/it]


  0%|▏                                                                   | 74/28816 [03:33<21:39:33,  2.71s/it]
{'loss': 1.3184, 'learning_rate': 1.7109826589595377e-05, 'epoch': 0.0}

  0%|▏                                                                   | 75/28816 [03:36<21:33:45,  2.70s/it]

  0%|▏                                                                   | 76/28816 [03:38<21:17:01,  2.67s/it]

  0%|▏                                                                   | 77/28816 [03:41<21:13:23,  2.66s/it]

  0%|▏                                                                   | 78/28816 [03:43<20:56:03,  2.62s/it]
{'loss': 1.3782, 'learning_rate': 1.8265895953757225e-05, 'epoch': 0.0}

  0%|▏                                                                   | 80/28816 [03:49<21:42:22,  2.72s/it]

  0%|▏                                                                   | 81/28816 [03:52<21:21:48,  2.68s/it]

  0%|▏                                                                   | 82/28816 [03:54<21:16:36,  2.67s/it]

  0%|▏                                                                   | 83/28816 [03:57<20:48:32,  2.61s/it]

  0%|▏                                                                   | 84/28816 [03:59<20:51:37,  2.61s/it]

  0%|▏                                                                   | 85/28816 [04:02<21:24:36,  2.68s/it]

  0%|▏                                                                   | 86/28816 [04:05<22:01:20,  2.76s/it]

  0%|▏                                                                   | 87/28816 [04:08<21:44:01,  2.72s/it]


  0%|▏                                                                   | 89/28816 [04:13<21:13:30,  2.66s/it]
{'loss': 1.2276, 'learning_rate': 2.0578034682080926e-05, 'epoch': 0.0}

  0%|▏                                                                   | 90/28816 [04:16<21:21:33,  2.68s/it]

  0%|▏                                                                   | 91/28816 [04:18<21:13:20,  2.66s/it]

  0%|▏                                                                   | 92/28816 [04:21<21:47:37,  2.73s/it]

  0%|▏                                                                   | 93/28816 [04:24<22:37:04,  2.83s/it]

  0%|▏                                                                   | 94/28816 [04:27<22:07:02,  2.77s/it]

  0%|▏                                                                   | 95/28816 [04:30<22:07:37,  2.77s/it]

  0%|▏                                                                   | 96/28816 [04:32<21:51:04,  2.74s/it]

  0%|▏                                                                   | 97/28816 [04:35<21:37:08,  2.71s/it]

  0%|▏                                                                   | 98/28816 [04:38<21:48:18,  2.73s/it]

  0%|▏                                                                   | 99/28816 [04:40<21:20:43,  2.68s/it]

  0%|▏                                                                  | 100/28816 [04:44<23:03:36,  2.89s/it]

  0%|▏                                                                  | 101/28816 [04:46<22:25:46,  2.81s/it]

  0%|▏                                                                  | 102/28816 [04:49<22:00:48,  2.76s/it]
[2024-08-26 16:11:18,171] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  0%|▏                                                                  | 103/28816 [04:53<23:44:09,  2.98s/it]

  0%|▏                                                                  | 104/28816 [04:55<23:29:25,  2.95s/it]

  0%|▏                                                                  | 105/28816 [04:58<22:58:17,  2.88s/it]

  0%|▏                                                                  | 106/28816 [05:01<22:34:55,  2.83s/it]

  0%|▏                                                                  | 107/28816 [05:03<22:05:51,  2.77s/it]

  0%|▎                                                                  | 108/28816 [05:06<22:26:08,  2.81s/it]

  0%|▎                                                                  | 109/28816 [05:10<23:37:33,  2.96s/it]


  0%|▎                                                                  | 111/28816 [05:15<22:40:13,  2.84s/it]
{'loss': 1.4171, 'learning_rate': 2.5664739884393062e-05, 'epoch': 0.0}

  0%|▎                                                                  | 112/28816 [05:18<23:34:18,  2.96s/it]

  0%|▎                                                                  | 113/28816 [05:21<22:37:32,  2.84s/it]

  0%|▎                                                                  | 114/28816 [05:24<22:23:16,  2.81s/it]


  0%|▎                                                                  | 116/28816 [05:29<21:59:50,  2.76s/it]
{'loss': 1.2518, 'learning_rate': 2.6820809248554914e-05, 'epoch': 0.0}

  0%|▎                                                                  | 117/28816 [05:32<21:20:17,  2.68s/it]

  0%|▎                                                                  | 118/28816 [05:34<21:20:46,  2.68s/it]

  0%|▎                                                                  | 119/28816 [05:37<21:06:25,  2.65s/it]

  0%|▎                                                                  | 120/28816 [05:40<21:09:28,  2.65s/it]

  0%|▎                                                                  | 121/28816 [05:43<21:51:53,  2.74s/it]

  0%|▎                                                                  | 122/28816 [05:45<21:26:34,  2.69s/it]


  0%|▎                                                                  | 124/28816 [05:51<23:09:14,  2.91s/it]
{'loss': 1.2035, 'learning_rate': 2.8670520231213876e-05, 'epoch': 0.0}

  0%|▎                                                                  | 125/28816 [05:54<22:37:22,  2.84s/it]

  0%|▎                                                                  | 126/28816 [05:57<22:13:21,  2.79s/it]

  0%|▎                                                                  | 127/28816 [05:59<22:18:58,  2.80s/it]

  0%|▎                                                                  | 128/28816 [06:02<22:35:23,  2.83s/it]


  0%|▎                                                                  | 130/28816 [06:07<21:11:47,  2.66s/it]
{'loss': 1.0856, 'learning_rate': 3.0057803468208097e-05, 'epoch': 0.0}


  0%|▎                                                                  | 132/28816 [06:13<22:29:25,  2.82s/it]
{'loss': 1.4245, 'learning_rate': 3.0520231213872835e-05, 'epoch': 0.0}

  0%|▎                                                                  | 133/28816 [06:16<21:53:55,  2.75s/it]


  0%|▎                                                                  | 135/28816 [06:21<21:50:50,  2.74s/it]
{'loss': 1.3878, 'learning_rate': 3.1213872832369946e-05, 'epoch': 0.0}

  0%|▎                                                                  | 136/28816 [06:24<22:31:58,  2.83s/it]

  0%|▎                                                                  | 137/28816 [06:27<21:47:47,  2.74s/it]

  0%|▎                                                                  | 138/28816 [06:29<21:47:19,  2.74s/it]

  0%|▎                                                                  | 139/28816 [06:32<22:17:44,  2.80s/it]

  0%|▎                                                                  | 140/28816 [06:35<22:12:59,  2.79s/it]

  0%|▎                                                                  | 141/28816 [06:38<21:56:15,  2.75s/it]


  0%|▎                                                                  | 143/28816 [06:43<21:51:30,  2.74s/it]
{'loss': 1.2581, 'learning_rate': 3.3063583815028905e-05, 'epoch': 0.0}

  0%|▎                                                                  | 144/28816 [06:46<22:17:51,  2.80s/it]

  1%|▎                                                                  | 145/28816 [06:49<21:54:11,  2.75s/it]

  1%|▎                                                                  | 146/28816 [06:52<22:28:30,  2.82s/it]

  1%|▎                                                                  | 147/28816 [06:54<21:48:35,  2.74s/it]

  1%|▎                                                                  | 148/28816 [06:57<22:03:22,  2.77s/it]

  1%|▎                                                                  | 149/28816 [07:00<22:27:43,  2.82s/it]

  1%|▎                                                                  | 150/28816 [07:03<22:05:15,  2.77s/it]

  1%|▎                                                                  | 151/28816 [07:06<22:35:25,  2.84s/it]
[2024-08-26 16:13:35,462] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  1%|▎                                                                  | 152/28816 [07:10<25:18:03,  3.18s/it]

  1%|▎                                                                  | 153/28816 [07:13<24:30:50,  3.08s/it]

  1%|▎                                                                  | 154/28816 [07:15<23:17:09,  2.92s/it]

  1%|▎                                                                  | 155/28816 [07:18<23:25:02,  2.94s/it]

  1%|▎                                                                  | 156/28816 [07:21<22:55:10,  2.88s/it]

  1%|▎                                                                  | 157/28816 [07:24<22:41:32,  2.85s/it]

  1%|▎                                                                  | 158/28816 [07:27<22:56:29,  2.88s/it]

  1%|▎                                                                  | 159/28816 [07:30<23:31:13,  2.95s/it]

  1%|▎                                                                  | 160/28816 [07:32<22:35:55,  2.84s/it]

  1%|▎                                                                  | 161/28816 [07:35<21:57:20,  2.76s/it]

  1%|▍                                                                  | 162/28816 [07:38<21:30:54,  2.70s/it]

  1%|▍                                                                  | 163/28816 [07:40<21:33:11,  2.71s/it]

  1%|▍                                                                  | 164/28816 [07:43<21:35:54,  2.71s/it]

  1%|▍                                                                  | 165/28816 [07:46<21:23:02,  2.69s/it]

  1%|▍                                                                  | 166/28816 [07:48<21:01:17,  2.64s/it]

  1%|▍                                                                  | 167/28816 [07:51<21:33:40,  2.71s/it]

  1%|▍                                                                  | 168/28816 [07:54<21:23:20,  2.69s/it]

  1%|▍                                                                  | 169/28816 [07:56<21:06:11,  2.65s/it]

  1%|▍                                                                  | 170/28816 [07:59<21:02:38,  2.64s/it]

  1%|▍                                                                  | 171/28816 [08:02<21:19:54,  2.68s/it]


  1%|▍                                                                  | 173/28816 [08:08<22:28:13,  2.82s/it]
{'loss': 1.2968, 'learning_rate': 4e-05, 'epoch': 0.01}

  1%|▍                                                                  | 174/28816 [08:10<21:59:54,  2.76s/it]

  1%|▍                                                                  | 175/28816 [08:13<22:13:28,  2.79s/it]

  1%|▍                                                                  | 176/28816 [08:16<21:55:49,  2.76s/it]


  1%|▍                                                                  | 178/28816 [08:22<23:01:14,  2.89s/it]
{'loss': 1.6696, 'learning_rate': 4.115606936416185e-05, 'epoch': 0.01}

  1%|▍                                                                  | 179/28816 [08:24<22:30:31,  2.83s/it]

  1%|▍                                                                  | 180/28816 [08:27<22:15:08,  2.80s/it]

  1%|▍                                                                  | 181/28816 [08:30<23:27:11,  2.95s/it]

  1%|▍                                                                  | 182/28816 [08:33<22:27:33,  2.82s/it]

  1%|▍                                                                  | 183/28816 [08:36<22:47:29,  2.87s/it]

  1%|▍                                                                  | 184/28816 [08:38<22:19:39,  2.81s/it]

  1%|▍                                                                  | 185/28816 [08:41<22:39:23,  2.85s/it]

  1%|▍                                                                  | 186/28816 [08:44<22:06:39,  2.78s/it]
[2024-08-26 16:15:13,925] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  1%|▍                                                                  | 187/28816 [08:48<25:37:14,  3.22s/it]

  1%|▍                                                                  | 188/28816 [08:51<24:49:58,  3.12s/it]

  1%|▍                                                                  | 189/28816 [08:54<24:36:32,  3.09s/it]

  1%|▍                                                                  | 190/28816 [08:57<23:21:16,  2.94s/it]

  1%|▍                                                                  | 191/28816 [09:00<22:58:53,  2.89s/it]

  1%|▍                                                                  | 192/28816 [09:02<22:25:06,  2.82s/it]

  1%|▍                                                                  | 193/28816 [09:05<22:19:58,  2.81s/it]

  1%|▍                                                                  | 194/28816 [09:08<23:35:44,  2.97s/it]

  1%|▍                                                                  | 195/28816 [09:11<23:11:21,  2.92s/it]

  1%|▍                                                                  | 196/28816 [09:14<22:33:57,  2.84s/it]

  1%|▍                                                                  | 197/28816 [09:16<21:56:08,  2.76s/it]

  1%|▍                                                                  | 198/28816 [09:19<22:07:59,  2.78s/it]

  1%|▍                                                                  | 199/28816 [09:22<22:00:18,  2.77s/it]

  1%|▍                                                                  | 200/28816 [09:25<22:00:58,  2.77s/it]

  1%|▍                                                                  | 201/28816 [09:27<21:29:36,  2.70s/it]

  1%|▍                                                                  | 202/28816 [09:30<21:05:57,  2.65s/it]

  1%|▍                                                                  | 203/28816 [09:33<23:29:03,  2.95s/it]

  1%|▍                                                                  | 204/28816 [09:36<23:34:33,  2.97s/it]


  1%|▍                                                                  | 206/28816 [09:42<22:15:24,  2.80s/it]
{'loss': 1.0862, 'learning_rate': 4.763005780346821e-05, 'epoch': 0.01}


  1%|▍                                                                  | 208/28816 [09:48<22:39:24,  2.85s/it]
{'loss': 1.051, 'learning_rate': 4.809248554913295e-05, 'epoch': 0.01}

  1%|▍                                                                  | 209/28816 [09:50<22:22:21,  2.82s/it]


  1%|▍                                                                  | 211/28816 [09:56<22:18:03,  2.81s/it]
{'loss': 1.2656, 'learning_rate': 4.878612716763006e-05, 'epoch': 0.01}

  1%|▍                                                                  | 212/28816 [09:58<21:51:19,  2.75s/it]


  1%|▍                                                                  | 214/28816 [10:04<21:02:26,  2.65s/it]
{'loss': 1.2865, 'learning_rate': 4.9479768786127164e-05, 'epoch': 0.01}

  1%|▍                                                                  | 215/28816 [10:06<21:05:21,  2.65s/it]


  1%|▌                                                                  | 217/28816 [10:12<21:18:53,  2.68s/it]
{'loss': 1.0171, 'learning_rate': 5.0173410404624275e-05, 'epoch': 0.01}

  1%|▌                                                                  | 218/28816 [10:14<21:02:26,  2.65s/it]


  1%|▌                                                                  | 220/28816 [10:20<21:37:07,  2.72s/it]
{'loss': 1.0943, 'learning_rate': 5.0867052023121385e-05, 'epoch': 0.01}

  1%|▌                                                                  | 221/28816 [10:23<21:56:06,  2.76s/it]


  1%|▌                                                                  | 223/28816 [10:28<21:08:45,  2.66s/it]
{'loss': 1.2812, 'learning_rate': 5.1560693641618496e-05, 'epoch': 0.01}

  1%|▌                                                                  | 224/28816 [10:31<21:27:29,  2.70s/it]


  1%|▌                                                                  | 226/28816 [10:36<21:11:34,  2.67s/it]
{'loss': 1.3003, 'learning_rate': 5.2254335260115606e-05, 'epoch': 0.01}

  1%|▌                                                                  | 227/28816 [10:38<21:08:21,  2.66s/it]

  1%|▌                                                                  | 228/28816 [10:41<21:36:27,  2.72s/it]

  1%|▌                                                                  | 229/28816 [10:44<21:47:20,  2.74s/it]

  1%|▌                                                                  | 230/28816 [10:47<22:19:06,  2.81s/it]

  1%|▌                                                                  | 231/28816 [10:50<23:29:12,  2.96s/it]


  1%|▌                                                                  | 233/28816 [10:56<22:30:20,  2.83s/it]
{'loss': 1.2134, 'learning_rate': 5.387283236994219e-05, 'epoch': 0.01}

  1%|▌                                                                  | 234/28816 [10:58<21:53:43,  2.76s/it]

  1%|▌                                                                  | 235/28816 [11:01<21:51:21,  2.75s/it]

  1%|▌                                                                  | 236/28816 [11:04<22:14:27,  2.80s/it]

  1%|▌                                                                  | 237/28816 [11:07<21:36:01,  2.72s/it]

  1%|▌                                                                  | 238/28816 [11:10<21:57:38,  2.77s/it]

  1%|▌                                                                  | 239/28816 [11:12<22:04:31,  2.78s/it]


  1%|▌                                                                  | 241/28816 [11:18<21:29:04,  2.71s/it]
{'loss': 1.0553, 'learning_rate': 5.5722543352601166e-05, 'epoch': 0.01}

  1%|▌                                                                  | 242/28816 [11:20<21:16:59,  2.68s/it]


  1%|▌                                                                  | 244/28816 [11:26<21:28:52,  2.71s/it]
{'loss': 1.0103, 'learning_rate': 5.6416184971098276e-05, 'epoch': 0.01}

  1%|▌                                                                  | 245/28816 [11:28<21:08:24,  2.66s/it]

  1%|▌                                                                  | 246/28816 [11:31<20:49:54,  2.62s/it]

  1%|▌                                                                  | 247/28816 [11:34<21:06:47,  2.66s/it]

  1%|▌                                                                  | 248/28816 [11:36<20:47:20,  2.62s/it]

  1%|▌                                                                  | 249/28816 [11:39<22:28:35,  2.83s/it]

  1%|▌                                                                  | 250/28816 [11:42<22:05:44,  2.78s/it]


  1%|▌                                                                  | 252/28816 [11:48<22:46:15,  2.87s/it]
{'loss': 0.8874, 'learning_rate': 5.8265895953757235e-05, 'epoch': 0.01}


  1%|▌                                                                  | 254/28816 [11:54<23:00:04,  2.90s/it]
